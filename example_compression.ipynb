{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application of the practical prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Preparation.Titanic_prep import model\n",
    "from utils import keras2torch_converter, compression_loop\n",
    "import keras\n",
    "import pandas as pd\n",
    "\n",
    "# titanic_keras = keras.models.load_model(\"Preparation/kaggle_titanic_model_new.keras\")\n",
    "titanic_keras = model\n",
    "\n",
    "titanic_torch = keras2torch_converter(titanic_keras)\n",
    "\n",
    "prune_data = pd.read_csv(\"Preparation/processed_to_prune.csv\")\n",
    "test_data = pd.read_csv(\"Preparation/processed_to_test_new.csv\")\n",
    "\n",
    "X_test = test_data.iloc[:, 1:]\n",
    "y_test = test_data.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compression.importance_based_pruning import Activation_based_pruning\n",
    "from Preparation.FashionMNIST_prep import trainloader_fashion, validationloader_fashion, testloader_fashion\n",
    "import pandas as pd\n",
    "\n",
    "fashion_model = Activation_based_pruning(784, 10)\n",
    "fashion_model.train(trainloader_fashion, 20, validationloader_fashion, 3)\n",
    "\n",
    "prune_data_fashion = pd.concat([pd.DataFrame(data)  for data, _ in validationloader_fashion])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### tmp ########\n",
    "import torch\n",
    "# torch.save(fashion_model.state_dict(), \"fashion_model_state_dict.pt\")\n",
    "\n",
    "from compression.importance_based_pruning import Activation_based_pruning\n",
    "from Preparation.FashionMNIST_prep import trainloader_fashion, validationloader_fashion, testloader_fashion\n",
    "import pandas as pd\n",
    "\n",
    "fashion_model = Activation_based_pruning(784, 10)\n",
    "fashion_model.load_state_dict(torch.load(\"fashion_model_state_dict.pt\"))\n",
    "prune_data_fashion = pd.concat([pd.DataFrame(data)  for data, _ in validationloader_fashion])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive compression without a layerthreshold\n",
    "compr_titanic_WO_layerthreshold = compression_loop(titanic_torch, prune_data, (X_test, y_test), layerthreshold= 0.0)\n",
    "compr_Fashion_WO_layerthreshold = compression_loop(fashion_model, prune_data_fashion, testloader_fashion, layerthreshold= 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compression with a absolute valued layerthreshold\n",
    "compr_titanic_abs_layerthreshold = compression_loop(titanic_torch, prune_data, (X_test, y_test), layerthreshold= 20)\n",
    "compr_Fashion_abs_layerthreshold = compression_loop(fashion_model, prune_data_fashion, testloader_fashion, layerthreshold= 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compression with a optimal valued layerthreshold\n",
    "compr_titanic_opt_layerthreshold = compression_loop(titanic_torch, prune_data, (X_test, y_test), layerthreshold= \"optimal\")\n",
    "compr_Fashion_opt_layerthreshold = compression_loop(fashion_model, prune_data_fashion, testloader_fashion, layerthreshold= \"optimal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting_functions import loss_plot, weight_plot, weight_loss_plot\n",
    "import matplotlib.pyplot as plt\n",
    "# change here for desired compression:\n",
    "plotting_data = compr_Fashion_WO_layerthreshold\n",
    "\n",
    "weight_plot(plotting_data)\n",
    "loss_plot(plotting_data)\n",
    "weight_loss_plot(plotting_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combination of compression techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "out = {}\n",
    "for compression_percentage in [0.25, 0.40, 0.50]:\n",
    "    for i in range(5):\n",
    "        model = Activation_based_pruning(784, 10)\n",
    "        model.load_state_dict(fashion_model.state_dict())\n",
    "        out[f\"{compression_percentage}compression_iteration{i + 1}\"] = model.train_n_prune(trainloader_fashion, 15, 0.70, compression_percentage, 0.05, val_loader = validationloader_fashion)\n",
    "        ## already saving the 60% sized model for later\n",
    "        if compression_percentage == 0.40:\n",
    "            import_pruned_model = copy.deepcopy(model)\n",
    "\n",
    "\n",
    "for key, value in out.items():\n",
    "    out[key] = list(value)\n",
    "\n",
    "\n",
    "for key, values in out.items():\n",
    "    values = pd.DataFrame(values, columns = (\"epochs\", \"train_acc\", \"val_acc\", \"model_size\"))\n",
    "    plt.plot(values.epochs, values.train_acc, label = key)\n",
    "    break\n",
    "\n",
    "plt.xlim((0, 15))\n",
    "plt.ylim(bottom = 0, top= 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_compression = compression_loop(import_pruned_model, prune_data_fashion, testloader_fashion, layerthreshold = \"optimal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_data = combined_compression\n",
    "\n",
    "weight_plot(plotting_data)\n",
    "loss_plot(plotting_data)\n",
    "weight_loss_plot(plotting_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml\n",
    "import keras\n",
    "import pandas as pd\n",
    "from Preparation.OpenML_prep import define_model, preprocess_features, preprocess_labels, train_validation_test_split, evaluate, acc\n",
    "from compression.compression import model_compression\n",
    "from utils import keras2torch_converter\n",
    "\n",
    "collection_weight_states = []\n",
    "\n",
    "def task_compression(task_id):\n",
    "    task = openml.tasks.get_task(task_id)\n",
    "    data = task.get_dataset()\n",
    "# preprocess data\n",
    "    X, y, _, _ = data.get_data(target=data.default_target_attribute)\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = train_validation_test_split(X, y)\n",
    "    _, X_val = preprocess_features(X_train, X_val)\n",
    "    X_train, X_test = preprocess_features(X_train, X_test)\n",
    "    _, y_val = preprocess_labels(y_train, y_val)\n",
    "    y_train, y_test = preprocess_labels(y_train, y_test)\n",
    "    print(f\"Data:{X.shape}\")\n",
    "# create model\n",
    "    model = define_model(X_train, y_train)\n",
    "# train model\n",
    "    model.fit(X_train, y_train, epochs = 10, batch_size = 32, verbose = 0)\n",
    "    print(\"trained\")\n",
    "# translate model\n",
    "    torch_model = keras2torch_converter(model)\n",
    "    collection_weight_states.append(torch_model.state_dict())\n",
    "    com_model075 = model_compression(torch_model)\n",
    "    com_model050 = model_compression(torch_model)\n",
    "    com_model025 = model_compression(torch_model)\n",
    "# compress model\n",
    "    com_model075.seq_fixed_size_compression(pd.DataFrame(X_val),rate = 0.75, threshold_stepsize = 0.1)\n",
    "    com_model050.seq_fixed_size_compression(pd.DataFrame(X_val),rate = 0.50, threshold_stepsize = 0.1)\n",
    "    com_model025.seq_fixed_size_compression(pd.DataFrame(X_val),rate = 0.25, threshold_stepsize = 0.1)\n",
    "    print(\"compressed\")\n",
    "# eval before and after\n",
    "    original_loss = evaluate(pd.DataFrame(X_test), pd.DataFrame(y_test), torch_model, acc)\n",
    "    compressed_loss075 = evaluate(pd.DataFrame(X_test), pd.DataFrame(y_test), com_model075, acc)\n",
    "    compressed_loss050 = evaluate(pd.DataFrame(X_test), pd.DataFrame(y_test), com_model050, acc)\n",
    "    compressed_loss025 = evaluate(pd.DataFrame(X_test), pd.DataFrame(y_test), com_model025, acc)\n",
    "    return data.name, original_loss, compressed_loss075, compressed_loss050, compressed_loss025\n",
    "\n",
    "keras.utils.set_random_seed(123)\n",
    "benchmark_suite = openml.study.get_suite(99)\n",
    "df_model_eval = pd.DataFrame(columns= [\"model\", \"original_loss\", \"compression_loss075\", \"compression_loss050\", \"compression_loss025\"])\n",
    "for idx, task_id in enumerate(benchmark_suite.tasks):\n",
    "    if idx in [25, 27, 40, 45, 63, 64, 65, 67, 68]:\n",
    "        print(f\"skipped {idx}\")\n",
    "        continue\n",
    "    df_model_eval.loc[df_model_eval.shape[0] + 1] = task_compression(task_id)\n",
    "    print(f\"{idx}/ {len(benchmark_suite.tasks) - 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class generalOpenML(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.layers = nn.Sequential()\n",
    "        self.layers.append(nn.Linear(in_dim, 64))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.Linear(64, 128))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.Linear(128, 128))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.Linear(128, 256))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.Linear(256, 256))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.Linear(256, out_dim))\n",
    "        if out_dim == 1:\n",
    "            self.layers.append(nn.Sigmoid())\n",
    "        else:\n",
    "            self.layers.append(nn.Softmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import check_provable_linearity\n",
    "\n",
    "for i in collection_weight_states:\n",
    "    in_dim =list(i.values())[0].shape[1]\n",
    "    out_dim = list(i.values())[-1].shape[0]\n",
    "\n",
    "    mod = generalOpenML(in_dim, out_dim)\n",
    "    mod.load_state_dict(i)\n",
    "\n",
    "    print(check_provable_linearity(mod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.boxplot([df_model_eval[\"original_loss\"], df_model_eval[\"compression_loss075\"], df_model_eval[\"compression_loss050\"], df_model_eval[\"compression_loss025\"]])\n",
    "ax.set_xticklabels([\"original\\nmodels\", \"75% sized\\nmodels\", \"50% sized\\nmodels\", \"25% sized\\nmodel\"])\n",
    "plt.ylim((0, 1))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "# plt.savefig(\"path/to/save.svg\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
